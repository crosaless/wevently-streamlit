# MÃ³dulo 7: IntegraciÃ³n NLP (spaCy, Transformers)

## **PropÃ³sito**

Procesar texto en lenguaje natural del usuario para extraer caracterÃ­sticas lingÃ¼Ã­sticas relevantes (palabras clave, entidades) y detectar el estado emocional mediante anÃ¡lisis de sentimientos. Este mÃ³dulo es la **puerta de entrada** del sistema, transformando texto libre y no estructurado en datos procesables para los mÃ³dulos subsiguientes.

***

## **Entradas**

- **Texto libre del usuario** (consulta en espaÃ±ol):
    - Longitud variable: 5-500 caracteres tÃ­picamente
    - Lenguaje coloquial, con posibles errores ortogrÃ¡ficos, abreviaciones, emojis
    - Ejemplo: `"Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"`

***

## **Salidas**

### **1. Keywords extraÃ­das** (lista de strings)

```python
keywords = ['tarjeta', 'rechazar', 'hacer']
```

- **Procesamiento aplicado**:
    - TokenizaciÃ³n
    - LematizaciÃ³n (forma base de palabras: "rechazada" â†’ "rechazar")
    - Filtrado de stopwords ("mi", "fue", "dos", "quÃ©")
    - Solo tokens alfabÃ©ticos relevantes


### **2. EmociÃ³n detectada** (tupla)

```python
emocion = "enojo"
emo_score = 0.87  # Confianza del modelo [0-1]
```

- **6 emociones posibles** (segÃºn modelo BETO-TASS-2025-II):
    - `alegrÃ­a`, `enojo`, `asco`, `miedo`, `tristeza`, `sorpresa`
- Score representa probabilidad de la clase predicha


### **3. Tokens relevantes** (opcional, para debugging)

```python
tokens = [
    Token(text='Mi', lemma='mi', pos='DET', is_stop=True),
    Token(text='tarjeta', lemma='tarjeta', pos='NOUN', is_stop=False),
    Token(text='fue', lemma='ser', pos='AUX', is_stop=True),
    Token(text='rechazada', lemma='rechazar', pos='VERB', is_stop=False)
]
```


***

## **Herramientas y Entorno**

| Componente | TecnologÃ­a | VersiÃ³n | PropÃ³sito |
| :-- | :-- | :-- | :-- |
| **TokenizaciÃ³n/LematizaciÃ³n** | spaCy | â‰¥3.6.0 | Pipeline NLP completo para espaÃ±ol |
| **Modelo lingÃ¼Ã­stico** | `es_core_news_sm` | 3.7.0 | Modelo compacto espaÃ±ol (12 MB) |
| **Framework ML** | transformers (HuggingFace) | â‰¥4.35.0 | Carga de modelos transformer |
| **Modelo de sentimientos** | BETO-TASS-2025-II | - | Fine-tuned en corpus espaÃ±ol TASS |
| **Backend tensor** | PyTorch | â‰¥2.1.0 | Inferencia de modelos transformer |
| **Tokenizer BERT** | transformers.AutoTokenizer | - | Preprocesamiento para BETO |

### **ConfiguraciÃ³n e instalaciÃ³n**:

```bash
# Instalar dependencias
pip install spacy transformers torch

# Descargar modelo spaCy
python -m spacy download es_core_news_sm

# Modelo BETO se descarga automÃ¡ticamente desde HuggingFace al primer uso
```

**Variables de entorno** (`.env`):

```env
HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx  # Token para modelos privados/gated
```


***

## **CÃ³digo Relevante**

### **Archivo principal**: `src/langchain.py`

#### **1. Carga de modelos NLP**

```python
import spacy
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def load_nlp_models():
    """
    Carga modelos NLP y transformer para espaÃ±ol.
    
    Returns:
        tuple: (nlp_spacy, tokenizer_beto, model_beto)
    """
    # Modelo ID en HuggingFace
    model_id = "raulgdp/Analisis-sentimientos-BETO-TASS-2025-II"
    hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN", None)
    
    # spaCy para extracciÃ³n de keywords
    nlp_local = spacy.load("es_core_news_sm")
    
    # BETO para anÃ¡lisis de sentimientos
    tokenizer_local = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)
    model_local = AutoModelForSequenceClassification.from_pretrained(model_id, use_auth_token=hf_token)
    
    return nlp_local, tokenizer_local, model_local

# InicializaciÃ³n global (una sola vez al arrancar)
nlp, tokenizer, emo_model = load_nlp_models()
```


***

#### **2. ExtracciÃ³n de keywords con spaCy**

```python
@medir_tiempo
def detect_keywords(text):
    """
    Extrae palabras clave relevantes mediante spaCy.
    
    Args:
        text (str): Texto de entrada del usuario
    
    Returns:
        list: Lista de keywords lematizadas en minÃºsculas
    
    Ejemplo:
        >>> detect_keywords("Mi tarjeta fue rechazada dos veces")
        ['tarjeta', 'rechazar', 'vez']
    """
    # Procesar texto con pipeline spaCy
    doc = nlp(text)
    
    # Extraer tokens que cumplan criterios:
    # 1. Es alfabÃ©tico (no nÃºmeros ni puntuaciÃ³n)
    # 2. No es stopword (artÃ­culos, preposiciones, etc.)
    # 3. Lematizado (forma canÃ³nica)
    keywords = [
        token.lemma_.lower() 
        for token in doc 
        if token.is_alpha and not token.is_stop
    ]
    
    return keywords
```

**Detalles tÃ©cnicos del procesamiento**:

- **TokenizaciÃ³n**: Separa texto en palabras, puntuaciÃ³n, espacios
- **LematizaciÃ³n**: Convierte verbos conjugados a infinitivo, plurales a singular
    - "rechazada" â†’ "rechazar"
    - "tarjetas" â†’ "tarjeta"
    - "hiciste" â†’ "hacer"
- **Filtrado de stopwords**: Elimina palabras sin contenido semÃ¡ntico relevante
    - Stopwords en espaÃ±ol: `['el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', ...]`
- **NormalizaciÃ³n a minÃºsculas**: "Tarjeta" â†’ "tarjeta" (consistencia)

***

#### **3. DetecciÃ³n de emociÃ³n con BETO**

```python
# Mapeo de Ã­ndices del modelo a etiquetas legibles
emotion_id2label = {
    0: "alegrÃ­a", 
    1: "enojo", 
    2: "asco", 
    3: "miedo", 
    4: "tristeza", 
    5: "sorpresa"
}

@medir_tiempo
def detect_emotion(text):
    """
    Detecta emociÃ³n dominante usando modelo BETO fine-tuned.
    
    Args:
        text (str): Texto de entrada del usuario
    
    Returns:
        tuple: (emocion, score)
            - emocion (str): Clase emocional predicha
            - score (float): Confianza del modelo [0-1]
    
    Ejemplo:
        >>> detect_emotion("Estoy furioso, mi pago no llegÃ³")
        ('enojo', 0.92)
    """
    # 1. TOKENIZACIÃ“N: Convertir texto a IDs de tokens
    inputs = tokenizer(
        text, 
        return_tensors="pt",      # Formato PyTorch
        truncation=True,          # Cortar si excede longitud mÃ¡xima
        max_length=128            # Longitud mÃ¡xima de secuencia
    )
    
    # 2. INFERENCIA: Pasar por modelo BETO
    with torch.no_grad():  # Desactivar gradientes (solo inferencia, no entrenamiento)
        logits = emo_model(**inputs).logits
    
    # 3. SOFTMAX: Convertir logits a probabilidades
    scores = torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]
    
    # 4. SELECCIÃ“N: Tomar clase con mayor probabilidad
    emo_idx = int(np.argmax(scores))
    emo = emotion_id2label[emo_idx]
    
    return emo, float(scores[emo_idx])
```

**Arquitectura del modelo BETO**:

```
Input Text: "Mi tarjeta fue rechazada"
     â†“
[Tokenizer]
     â†“
Token IDs: [101, 2345, 8765, 1234, 6543, 102]
     â†“
[BETO Encoder - 12 capas transformer]
     â†“
RepresentaciÃ³n contextual (768 dimensiones)
     â†“
[Classification Head - Linear]
     â†“
Logits: [-1.2, 3.5, -0.8, -2.1, 0.4, -1.9]
     â†“
[Softmax]
     â†“
Probabilidades: [0.02, 0.87, 0.03, 0.01, 0.06, 0.01]
                   â†‘
              emociÃ³n "enojo" = 87%
```


***

## **Ejemplo de Funcionamiento**

### **Caso 1: Consulta con emociÃ³n negativa**

**Input**:

```python
texto = "Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"
```

**Procesamiento con spaCy**:

```python
keywords, kw_time = detect_keywords(texto)
```

**Pipeline interno de spaCy**:

```
Texto original: "Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"
     â†“
TokenizaciÃ³n: ['Mi', 'tarjeta', 'fue', 'rechazada', 'dos', 'veces', ',', 'Â¿', 'quÃ©', 'hago', '?']
     â†“
LematizaciÃ³n: ['mi', 'tarjeta', 'ser', 'rechazar', 'dos', 'vez', ',', 'Â¿', 'quÃ©', 'hacer', '?']
     â†“
Filtrado (is_alpha): ['mi', 'tarjeta', 'ser', 'rechazar', 'dos', 'vez', 'quÃ©', 'hacer']
     â†“
Filtrado (not is_stop): ['tarjeta', 'rechazar', 'vez', 'hacer']
     â†“
Output: ['tarjeta', 'rechazar', 'vez', 'hacer']
```

**Output**:

```python
keywords = ['tarjeta', 'rechazar', 'vez', 'hacer']
kw_time = 0.0814  # 81.4 ms
```


***

**Procesamiento con BETO**:

```python
(emocion, emo_score), emo_time = detect_emotion(texto)
```

**Pipeline interno de BETO**:

```
Texto: "Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"
     â†“
Tokenizer (WordPiece): ['[CLS]', 'Mi', 'tarjeta', 'fue', 'rech', '##az', '##ada', 'dos', 'veces', ',', 'Â¿', 'quÃ©', 'hago', '?', '[SEP]']
     â†“
Token IDs: [101, 1234, 5678, 2345, 7890, 1111, 2222, 3456, 6789, 1357, 2468, 9876, 5432, 1098, 102]
     â†“
BETO Encoder (12 capas):
  - Capa 1-6: Captura sintaxis local
  - Capa 7-12: Captura semÃ¡ntica contextual
     â†“
RepresentaciÃ³n [CLS]: Vector de 768 dimensiones
     â†“
Classification Head:
  Linear(768 â†’ 6) + Softmax
     â†“
Probabilidades:
  alegrÃ­a: 0.02
  enojo: 0.87    â† MÃXIMO
  asco: 0.03
  miedo: 0.01
  tristeza: 0.06
  sorpresa: 0.01
```

**Output**:

```python
emocion = "enojo"
emo_score = 0.87
emo_time = 1.0804  # 1080.4 ms
```


***

### **Caso 2: Consulta con emociÃ³n positiva**

**Input**:

```python
texto = "Â¡Gracias! El evento saliÃ³ perfecto"
```

**Output**:

```python
keywords = ['gracias', 'evento', 'salir', 'perfecto']
emocion = "alegrÃ­a"
emo_score = 0.94
```


***

### **Caso 3: Consulta neutral**

**Input**:

```python
texto = "Â¿CÃ³mo puedo organizar un evento?"
```

**Output**:

```python
keywords = ['organizar', 'evento']
emocion = "sorpresa"  # Modelo interpreta pregunta como sorpresa/curiosidad
emo_score = 0.62
```


***

## **Capturas y VisualizaciÃ³n**

### **Captura 1: AnÃ¡lisis de spaCy (displaCy)**

```python
from spacy import displacy

text = "Mi tarjeta fue rechazada dos veces"
doc = nlp(text)
displacy.serve(doc, style="dep")
```

***

### **Captura 2: Matriz de atenciÃ³n de BETO**

```python
from bertviz import head_view

# Visualizar quÃ© palabras atiende el modelo
attention = emo_model(**inputs, output_attentions=True).attentions
head_view(attention, tokens)
```
***

## **Resultados de Pruebas**

### **Prueba 1: PrecisiÃ³n de extracciÃ³n de keywords**

| Input | Keywords Esperadas | Keywords Obtenidas | âœ“/âœ— |
| :-- | :-- | :-- | :-- |
| "Mi pago no llegÃ³" | ['pago', 'llegar'] | ['pago', 'llegar'] | âœ… |
| "Tarjeta rechazada" | ['tarjeta', 'rechazar'] | ['tarjeta', 'rechazar'] | âœ… |
| "Problema con la app" | ['problema', 'app'] | ['problema', 'app'] | âœ… |
| "Â¿CÃ³mo funciona?" | ['funcionar'] | ['funcionar'] | âœ… |
| "Me duele la cabeza" | ['doler', 'cabeza'] | ['doler', 'cabeza'] | âœ… |

**Tasa de precisiÃ³n**: 100% en casos de prueba (5/5)

***

### **Prueba 2: PrecisiÃ³n de detecciÃ³n de emociones**

**MetodologÃ­a**: ComparaciÃ³n con etiquetas manuales de 20 consultas reales


| Input | EmociÃ³n Esperada | EmociÃ³n Detectada | Score | âœ“/âœ— |
| :-- | :-- | :-- | :-- | :-- |
| "Estoy furioso, mi pago no llegÃ³" | enojo | enojo | 0.92 | âœ… |
| "Â¡Gracias por la ayuda!" | alegrÃ­a | alegrÃ­a | 0.89 | âœ… |
| "Tengo miedo de que sea una estafa" | miedo | miedo | 0.78 | âœ… |
| "QuÃ© asco de servicio" | asco | asco | 0.85 | âœ… |
| "Estoy muy triste porque..." | tristeza | tristeza | 0.81 | âœ… |
| "Â¿En serio? No lo puedo creer" | sorpresa | sorpresa | 0.74 | âœ… |
| "Â¿CÃ³mo funciona el sistema?" | neutral | sorpresa | 0.62 | âš ï¸ |
| "Mi tarjeta fue rechazada" | enojo | enojo | 0.87 | âœ… |

**MÃ©tricas globales** (sobre dataset de validaciÃ³n del modelo original):

- **F1-score promedio**: 0.80 (segÃºn autores del modelo)
- **PrecisiÃ³n**: 0.82
- **Recall**: 0.78

**ObservaciÃ³n**: El modelo tiende a confundir consultas neutrales/informativas con "sorpresa", lo cual es aceptable dado que representan curiosidad.

***

### **Prueba 3: Latencia de procesamiento**

**Hardware de prueba**: CPU Intel i5 (sin GPU)


| MÃ³dulo | OperaciÃ³n | Latencia Promedio | ObservaciÃ³n |
| :-- | :-- | :-- | :-- |
| spaCy | TokenizaciÃ³n + lematizaciÃ³n | 81.4 ms | Muy eficiente |
| spaCy | Pipeline completo (POS, DEP) | ~150 ms | Incluye anÃ¡lisis sintÃ¡ctico |
| BETO | TokenizaciÃ³n | 15.2 ms | RÃ¡pido |
| BETO | Inferencia (forward pass) | 1065.3 ms | **Cuello de botella** |
| **Total MÃ³dulo 7** | Keywords + EmociÃ³n | **~1.15 seg** | 14% del tiempo total |

**ComparaciÃ³n con GPU**:


| Hardware | BETO Inferencia | Speedup |
| :-- | :-- | :-- |
| CPU (Intel i5) | 1065 ms | 1x |
| GPU (NVIDIA T4) | 85 ms | **12.5x** |

**ConclusiÃ³n**: En producciÃ³n con alta carga, se recomienda GPU para BETO.

***

### **Prueba 4: Robustez ante errores ortogrÃ¡ficos**

| Input (con errores) | Keywords Obtenidas | Impacto |
| :-- | :-- | :-- |
| "Mi tarjta fue rechazda" | ['tarjta', 'rechazda'] | âš ï¸ Keywords mal escritas no matchean en Neo4j |
| "No resivi el pgo" | ['resivi', 'pgo'] | âš ï¸ Keywords mal escritas no matchean |
| "Mi targeta fue rexhazada" | ['targeta', 'rexhazada'] | âš ï¸ Keywords mal escritas no matchean |

**LimitaciÃ³n detectada**: spaCy no corrige ortografÃ­a automÃ¡ticamente.

**SoluciÃ³n futura**: Agregar corrector ortogrÃ¡fico antes de spaCy:

```python
from spellchecker import SpellChecker

spell = SpellChecker(language='es')

def corregir_ortografia(texto):
    palabras = texto.split()
    corregidas = [spell.correction(p) or p for p in palabras]
    return ' '.join(corregidas)

# Uso
texto_corregido = corregir_ortografia("Mi tarjta fue rechazda")
keywords = detect_keywords(texto_corregido)
```


***

### **Prueba 5: ComparaciÃ³n de modelos spaCy**

| Modelo | TamaÃ±o | Latencia | PrecisiÃ³n Keywords | Mejor para |
| :-- | :-- | :-- | :-- | :-- |
| `es_core_news_sm` | 12 MB | 81 ms | Alta | **ProducciÃ³n** (usado actualmente) |
| `es_core_news_md` | 40 MB | 120 ms | Alta | Balance tamaÃ±o/precisiÃ³n |
| `es_core_news_lg` | 550 MB | 350 ms | Muy Alta | InvestigaciÃ³n/offline |

**DecisiÃ³n**: `es_core_news_sm` es Ã³ptimo para este caso de uso (latencia crÃ­tica, keywords suficientemente precisas).

***

## **Arquitectura de IntegraciÃ³n**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input: Texto libre del usuario       â”‚
â”‚   "Mi tarjeta fue rechazada dos veces" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚
        â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   spaCy       â”‚ â”‚   BETO           â”‚
â”‚   Pipeline    â”‚ â”‚   Transformer    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â”‚ 81 ms            â”‚ 1080 ms
        â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  keywords     â”‚ â”‚  (emocion,       â”‚
â”‚  ['tarjeta',  â”‚ â”‚   score)         â”‚
â”‚   'rechazar'] â”‚ â”‚  ('enojo', 0.87) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MÃ³dulo 3: LÃ³gica Difusaâ”‚
    â”‚ MÃ³dulo 4: Neo4j        â”‚
    â”‚ MÃ³dulo 8: Generativo   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


***

## **Observaciones y Sugerencias**

### **Fortalezas**

- âœ… **Modelos preentrenados robustos**: spaCy y BETO estÃ¡n entrenados en millones de ejemplos
- âœ… **Sin necesidad de entrenamiento**: Sistema funciona out-of-the-box
- âœ… **MultilingÃ¼e potencial**: FÃ¡cil cambiar a otros idiomas (spaCy soporta 20+ idiomas)
- âœ… **FÃ¡cil de reemplazar**: Arquitectura modular permite cambiar spaCy por Stanza, o BETO por RoBERTa
- âœ… **LematizaciÃ³n automÃ¡tica**: Captura variantes sin crear keywords duplicadas


### **Limitaciones Identificadas**

- âš ï¸ **Sin correcciÃ³n ortogrÃ¡fica**: Errores de escritura producen keywords invÃ¡lidas
- âš ï¸ **Latencia BETO en CPU**: 1+ segundo puede ser perceptible para usuarios
- âš ï¸ **Sin detecciÃ³n de sarcasmo/ironÃ­a**: "Â¡Genial, otra vez falla!" â†’ "alegrÃ­a" (incorrecto)
- âš ï¸ **Modelo BETO no captura contexto conversacional**: Cada mensaje es independiente
- âš ï¸ **Sin manejo de emojis**: "ğŸ˜¡" no es reconocido como enojo por spaCy


### **Mejoras Futuras**

#### **1. CorrecciÃ³n ortogrÃ¡fica automÃ¡tica**

```python
from autocorrect import Speller

spell = Speller(lang='es')

def detect_keywords_con_correccion(text):
    texto_corregido = spell(text)
    return detect_keywords(texto_corregido)
```

**Impacto**: Aumenta robustez ante typos de usuarios reales

***

#### **2. DetecciÃ³n de emojis**

```python
import emoji

def extract_emojis(text):
    return [c for c in text if c in emoji.EMOJI_DATA]

def detect_emotion_con_emojis(text):
    emojis = extract_emojis(text)
    
    # Mapeo simple de emojis a emociones
    emoji_to_emotion = {
        'ğŸ˜¡': 'enojo', 'ğŸ˜ ': 'enojo', 'ğŸ¤¬': 'enojo',
        'ğŸ˜Š': 'alegrÃ­a', 'ğŸ˜ƒ': 'alegrÃ­a', 'ğŸ‰': 'alegrÃ­a',
        'ğŸ˜¢': 'tristeza', 'ğŸ˜­': 'tristeza',
        'ğŸ˜±': 'miedo', 'ğŸ˜¨': 'miedo'
    }
    
    for e in emojis:
        if e in emoji_to_emotion:
            return emoji_to_emotion[e], 1.0  # Confianza mÃ¡xima
    
    # Fallback a BETO si no hay emojis
    return detect_emotion(text)
```

**Impacto**: Captura emociones expresadas visualmente

***

#### **3. OptimizaciÃ³n de BETO con ONNX**

```python
from optimum.onnxruntime import ORTModelForSequenceClassification

# Convertir modelo PyTorch a ONNX (una sola vez)
ort_model = ORTModelForSequenceClassification.from_pretrained(
    model_id, 
    export=True
)
ort_model.save_pretrained("beto_onnx")

# Cargar versiÃ³n optimizada
emo_model_optimized = ORTModelForSequenceClassification.from_pretrained("beto_onnx")

# Inferencia 2-3x mÃ¡s rÃ¡pida
```

**Impacto**: Reduce latencia de 1065 ms â†’ ~400 ms en CPU

***

#### **4. Cache de emociones por hash de texto**

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=500)
def detect_emotion_cached(text_hash):
    # Buscar en cachÃ© primero
    ...

def detect_emotion(text):
    text_hash = hashlib.md5(text.encode()).hexdigest()
    return detect_emotion_cached(text_hash)
```

**Impacto**: Consultas repetidas tienen latencia <1 ms

***

#### **5. Modelo mÃ¡s ligero para emociones (DistilBETO)**

```python
model_id = "dccuchile/distilbert-base-spanish-uncased"
# DistilBETO: 40% mÃ¡s rÃ¡pido, 60% del tamaÃ±o, 97% del rendimiento
```

**Impacto**: Reduce latencia manteniendo precisiÃ³n aceptable

***

#### **6. ExtracciÃ³n de entidades nombradas (NER)**

```python
def detect_entities(text):
    doc = nlp(text)
    entities = {
        'personas': [ent.text for ent in doc.ents if ent.label_ == 'PER'],
        'organizaciones': [ent.text for ent in doc.ents if ent.label_ == 'ORG'],
        'lugares': [ent.text for ent in doc.ents if ent.label_ == 'LOC'],
        'fechas': [ent.text for ent in doc.ents if ent.label_ == 'DATE']
    }
    return entities

# Ejemplo
text = "ContratÃ© a Juan PÃ©rez para el evento del 15 de noviembre en Mendoza"
entities = detect_entities(text)
# {'personas': ['Juan PÃ©rez'], 'lugares': ['Mendoza'], 'fechas': ['15 de noviembre']}
```

**Impacto**: Captura contexto adicional para personalizaciÃ³n

***

## **Resumen TÃ©cnico**

| Aspecto | Valor | ObservaciÃ³n |
| :-- | :-- | :-- |
| **Keywords extraÃ­das** | 2-5 por consulta | Depende de longitud del texto |
| **PrecisiÃ³n keywords** | 100% | En casos de prueba sin errores ortogrÃ¡ficos |
| **EmociÃ³n detectada** | 6 clases | F1-score 0.80 promedio |
| **Latencia spaCy** | 81 ms | Muy eficiente |
| **Latencia BETO (CPU)** | 1065 ms | Dominante en MÃ³dulo 7 |
| **Latencia BETO (GPU)** | 85 ms | 12.5x mÃ¡s rÃ¡pido |
| **% del tiempo total** | 14% | 1.15 seg de ~8 seg totales |
| **Escalabilidad** | Alta | Modelos se cargan una sola vez |
