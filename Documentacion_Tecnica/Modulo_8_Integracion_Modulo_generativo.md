# MÃ³dulo 8: IntegraciÃ³n Generativa (Ollama + LangChain)

## PropÃ³sito

Generar la **respuesta final en lenguaje natural**, ajustando el tono, estilo y contenido segÃºn el contexto recuperado de la base de conocimiento (Neo4j), el rol del usuario y su estado emocional. Este mÃ³dulo es la **interfaz de salida del sistema**, transformando datos estructurados en texto conversacional coherente, empÃ¡tico y personalizado.

**Roles clave:**

1. **Seleccionador de soluciones:** Usa LLM para elegir la mejor soluciÃ³n de mÃºltiples candidatos de Neo4j
2. **Generador de respuestas:** Crea texto final adaptado al contexto emocional y rol del usuario

***

## Entradas

### 1. Datos del contexto (desde mÃ³dulos anteriores)

```python
# Del MÃ³dulo 5 (Planificador)
plan = {
    "categoria_ml": "Rechazo_Tarjeta",
    "confianza_ml": 0.45
}

# Del MÃ³dulo 7 (NLP)
keywords = ["tarjeta", "rechazar"]
emocion = "enojo"
emo_score = 0.87

# Del MÃ³dulo 3 (LÃ³gica Difusa)
confianza_fuzzy = 0.90

# Del MÃ³dulo 4 (Neo4j)
result = [
    {
        "tipoproblema": "Tarjeta rechazada",
        "solucion": "Verifique los datos de su tarjeta...",
        "confianza": 0.85,
        "matchedkeywords": ["tarjeta", "rechazar"],
        "matchedcount": 2
    },
    # ... mÃºltiples resultados posibles
]

# Datos del usuario
pregunta = "Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"
tipousuario = "Organizador"
```


***

## Salidas

### 1. SelecciÃ³n de mejor soluciÃ³n (desde `elegir_mejor_solucion_con_llm()`)

```python
def elegir_mejor_solucion_con_llm(user_message, all_results, categoria_ml, emocion, llm):
    """
    Usa LLM para seleccionar la mejor soluciÃ³n de mÃºltiples candidatos.
    
    Returns:
        tuple: (tipoproblema, solucion, elegido_result, justificacion)
    """
```

**Output:**

```python
tipoproblema_llm = "Tarjeta rechazada"
solucion_llm = "Verifique los datos de su tarjeta e intente nuevamente."
elegido_result = {...}  # Resultado completo seleccionado
justificacion_llm = "OpciÃ³n 1 ... justificaciÃ³n breve del LLM"
```


***

### 2. Respuesta final generada (desde `generar_respuesta_streamlit()`)

```python
respuesta: str  # Texto en lenguaje natural
```

**Ejemplo de respuesta:**

```
Hola estimado organizador! Entendemos tu frustraciÃ³n cuando una tarjeta es 
rechazada durante el proceso de pago.

Para resolver este problema, te recomendamos los siguientes pasos:

1. Verifica los datos de tu tarjeta: AsegÃºrate de que el nÃºmero de tarjeta, 
   fecha de vencimiento y cÃ³digo CVV estÃ©n correctamente ingresados.

2. Confirma el saldo disponible: Contacta con tu banco para verificar que tu 
   tarjeta tenga fondos suficientes y no estÃ© bloqueada.

3. Intenta con otra tarjeta: Si el problema persiste, prueba con un mÃ©todo 
   de pago alternativo.

4. Contacta a soporte si continÃºa fallando: Si despuÃ©s de estos pasos sigues 
   teniendo problemas, escrÃ­benos a wevently.empresa@gmail.com.

Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. 
Cualquier duda no dudes en consultarme.

Respuesta recomendada por nuestro sistema.
```

**CaracterÃ­sticas de la respuesta:**

- âœ… Saludo personalizado por rol
- âœ… Tono ajustado a emociÃ³n detectada
- âœ… Estructura clara (pasos numerados)
- âœ… InformaciÃ³n de contacto de soporte
- âœ… Extras especÃ­ficos por rol
- âœ… Post-data segÃºn nivel de confianza
- âœ… Longitud apropiada (150-400 palabras)

***

## Herramientas y Entorno

| Componente | TecnologÃ­a | VersiÃ³n | PropÃ³sito |
| :-- | :-- | :-- | :-- |
| **Proveedor LLM** | Ollama Cloud | - | Inferencia de modelos LLM en la nube |
| **Modelo** | `gpt-oss:20b-cloud` | 20B parÃ¡metros | Modelo generativo |
| **Framework integraciÃ³n** | `langchain-ollama` | 0.1.0+ | Wrapper Python para Ollama |
| **Base LangChain** | `langchain` | 0.1.0+ | OrquestaciÃ³n de LLMs |
| **Lenguaje** | Python | 3.9+ | ImplementaciÃ³n |


***

## ConfiguraciÃ³n

### InstalaciÃ³n

```bash
pip install langchain langchain-ollama
```


### Variables de entorno (opcional)

```bash
# .env
OLLAMA_BASE_URL=https://ollama.com
OLLAMA_API_KEY=tu_api_key  # Si requiere autenticaciÃ³n
```


***

## CÃ³digo Relevante

### 1. InicializaciÃ³n del cliente Ollama

```python
from langchain_ollama import OllamaLLM
import os

# Configurar cliente LLM
llm = OllamaLLM(
    model="gpt-oss:20b-cloud",
    base_url="https://ollama.com"
)

# Opcional: configurar API key si el modelo lo requiere
if os.getenv('OLLAMA_API_KEY'):
    os.environ['OLLAMA_API_KEY'] = os.getenv('OLLAMA_API_KEY')
```


***

### 2. Detalles de personalizaciÃ³n por rol

```python
roledetails = {
    "Organizador": {
        "saludo": "Hola estimado organizador! ",
        "tono": "empÃ¡tico y resolutivo",
        "extra": "Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. Cualquier duda no dudes en consultarme. "
    },
    "Prestador": {
        "saludo": "Hola prestador, ",
        "tono": "enfocado en apoyo operativo y resolutivo",
        "extra": "No olvides mantener tu perfil y disponibilidad actualizados para evitar inconvenientes. "
    },
    "Propietario": {
        "saludo": "Hola propietario, ",
        "tono": "informativo, estratÃ©gico y resolutivo",
        "extra": "No olvides mantener tu perfil y disponibilidad actualizados para evitar inconvenientes. "
    }
}
```


***

### 3. Mapeo de emociÃ³n a tono de respuesta

```python
EMOTION_TO_TONE = {
    "alegrÃ­a": "positivo, amable y orientado a soluciones",
    "enojo": "serio, conciliador y orientado a soluciones",
    "asco": "profesional y directo",
    "miedo": "tranquilizador, empÃ¡tico y claro",
    "tristeza": "consolador, empÃ¡tico y paciente",
    "sorpresa": "informativo y claro"
}
```


***

### 4. FunciÃ³n de selecciÃ³n con LLM

```python
def elegir_mejor_solucion_con_llm(user_message, all_results, categoria_ml, emocion, llm):
    """
    Usa LLM para seleccionar la mejor soluciÃ³n de mÃºltiples candidatos de Neo4j.
    
    Args:
        user_message (str): Mensaje original del usuario
        all_results (list): Lista de resultados de Neo4j
        categoria_ml (str): CategorÃ­a predicha por ML
        emocion (str): EmociÃ³n detectada por BETO
        llm: Instancia del modelo LLM
    
    Returns:
        tuple: (tipoproblema, solucion, elegido_result, justificacion)
    """
    if not all_results:
        return None, None, None, "No hay soluciones candidatas en la base de conocimiento."
    
    # Formatear opciones para el LLM
    candidates_text = "\n".join([
        f"OpciÃ³n {i+1}: Tipo={r.get('tipoproblema','')}, "
        f"SoluciÃ³n={r.get('solucion','')}, "
        f"Confianza={r.get('confianza',0):.2f}, "
        f"Keywords={r.get('matchedkeywords',[])}"
        for i, r in enumerate(all_results)
    ])
    
    # Construir prompt para selecciÃ³n
    selection_prompt = f"""Como capa intermedia de un proceso de decisiÃ³n para ofrecer la mejor soluciÃ³n al problema/consulta del usuario, debes elegir cual es la mejor soluciÃ³n de las ofrecidas para el problema que plantea el usuario. No modifiques la soluciÃ³n ni el tipo de problema

Mensaje del usuario: {user_message}
CategorÃ­a ML: {categoria_ml}
EmociÃ³n detectada: {emocion}

Soluciones candidatas:
{candidates_text}

EvalÃºa todas las opciones y elige la mÃ¡s relevante para el mensaje y emociÃ³n del usuario.
Elige SOLO la opciÃ³n mÃ¡s relevante para el mensaje y emociÃ³n del usuario. Responde exactamente con "OpciÃ³n X" seguido de una justificaciÃ³n breve. Si varias opciones son similares, desempata por cantidad de keywords y confianza."""
    
    # Invocar LLM
    respuesta = llm.invoke(selection_prompt)
    print(respuesta)
    
    # Extraer Ã­ndice de opciÃ³n elegida (regex robusta)
    import re
    match = re.search(r'OpciÃ³n\s+(\d+)', respuesta)
    
    if not match:
        return None, None, None, "No se pudo determinar opciÃ³n de LLM. JustificaciÃ³n: " + respuesta
    
    idx = int(match.group(1)) - 1
    
    if idx < 0 or idx >= len(all_results):
        return None, None, None, "Ãndice elegido fuera de rango por el LLM."
    
    elegido = all_results[idx]
    justificacion = respuesta
    
    return elegido.get('tipoproblema'), elegido.get('solucion'), elegido, justificacion
```


***

### 5. ConstrucciÃ³n y ejecuciÃ³n del prompt final

```python
def generar_respuesta_streamlit(pregunta, tipousuario="Prestador", debug=False):
    """
    FunciÃ³n principal que genera respuesta usando LLM.
    Orquesta todos los mÃ³dulos y construye prompt contextualizado.
    """
    # ... procesamiento previo (keywords, emociÃ³n, fuzzy, Neo4j) ...
    
    # SELECCIÃ“N DE MEJOR SOLUCIÃ“N CON LLM
    tipoproblema_llm, solucion_llm, elegido_result, justificacion_llm = \
        elegir_mejor_solucion_con_llm(pregunta, result, plan["categoria_ml"], emocion, llm)
    
    # OBTENER DETALLES DE PERSONALIZACIÃ“N
    rd = roledetails.get(tipousuario, roledetails["Prestador"])
    emotion_tone = EMOTION_TO_TONE.get(emocion, rd.get('tono', 'neutral'))
    
    # CONSTRUCCIÃ“N DEL PROMPT
    prompt_llm = f"""Como asistente del sistema Wevently para la organizaciÃ³n de eventos privados donde organizadores, prestadores de servicios y propietarios de lugar operan, contesta a la pregunta del usuario.

{rd['saludo']}Se detectÃ³ el problema: {tipoproblema_llm}.
SoluciÃ³n sugerida: {solucion_llm} {justificacion_llm}.
Por favor responde en un tono {emotion_tone}.

CategorÃ­a ML: {plan['categoria_ml']}, EmociÃ³n detectada: {emocion}, score emociÃ³n: {emo_score:.2f}, confianza ML: {plan['confianza_ml']:.2f}, confianza fuzzy: {confianza_fuzzy:.2f}.

Mensaje original: {pregunta}

{rd['extra']}{postdata}"""
    
    # MEDICIÃ“NDEL DE LATENCIA
    inicio_llm = time.time()
    
    # INVOCACIÃ“N DEL LLM
    respuesta = llm.invoke(prompt_llm)
    
    llm_time = time.time() - inicio_llm
    logger.info(f"LLM respuesta generada {llm_time:.4f}s")
    
    return respuesta, keywords, emocion, confianza_fuzzy
```


***

## Ejemplos de Funcionamiento

### Caso 1: Organizador con problema de pago (emociÃ³n: enojo)

**Input al LLM:**

```python
tipousuario = "Organizador"
tipoproblema_llm = "Tarjeta rechazada"
solucion_llm = "Verifique los datos de su tarjeta e intente nuevamente."
emocion = "enojo"
emo_score = 0.87
confianza_fuzzy = 0.90
pregunta = "Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?"
```

**Prompt generado:**

```
Como asistente del sistema Wevently para la organizaciÃ³n de eventos privados donde organizadores, prestadores de servicios y propietarios de lugar operan, contesta a la pregunta del usuario.

Hola estimado organizador! Se detectÃ³ el problema: Tarjeta rechazada.
SoluciÃ³n sugerida: Verifique los datos de su tarjeta e intente nuevamente. OpciÃ³n 1 ...
Por favor responde en un tono serio, conciliador y orientado a soluciones.

CategorÃ­a ML: Rechazo_Tarjeta, EmociÃ³n detectada: enojo, score emociÃ³n: 0.87, confianza ML: 0.45, confianza fuzzy: 0.90.

Mensaje original: Mi tarjeta fue rechazada dos veces, Â¿quÃ© hago?

Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. Cualquier duda no dudes en consultarme. Respuesta recomendada por nuestro sistema.
```

**Respuesta del LLM:**

```
Hola estimado organizador! Entiendo perfectamente tu frustraciÃ³n cuando una tarjeta es 
rechazada durante el pago. Trabajemos juntos para resolver esto:

Pasos inmediatos:
1. Verifica los datos ingresados:
   - NÃºmero de tarjeta completo
   - Fecha de vencimiento (MM/AA)
   - CÃ³digo CVV (3 dÃ­gitos al reverso)
   - Nombre tal como aparece en la tarjeta

2. Confirma con tu banco:
   - Saldo disponible suficiente
   - Sin bloqueos por seguridad
   - LÃ­mite de compra no excedido

3. Prueba alternativas:
   - Otra tarjeta de dÃ©bito/crÃ©dito
   - Diferentes navegadores o dispositivos

Si el problema persiste:
Contacta a nuestro equipo de soporte en wevently.empresa@gmail.com con:
- ID del evento
- Ãšltimos 4 dÃ­gitos de la tarjeta
- Captura del mensaje de error

Responderemos en menos de 24 horas hÃ¡biles.

Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. Cualquier duda 
no dudes en consultarme.

Estamos aquÃ­ para ayudarte!
```

**Latencia:** 4288 ms (~4.3 segundos)

***

### Caso 2: Prestador con demora de pago (emociÃ³n: enojo)

**Input:**

```python
tipousuario = "Prestador"
tipoproblema_llm = "Demora en acreditaciÃ³n"
solucion_llm = "El dinero debe acreditarse en 24-48hs segÃºn tÃ©rminos y condiciones."
emocion = "enojo"
emo_score = 0.92
pregunta = "TodavÃ­a no recibÃ­ el pago y necesito una soluciÃ³n urgente"
```

**Respuesta del LLM:**

```
Hola prestador, Entendemos tu preocupaciÃ³n y la urgencia de tu situaciÃ³n. Veamos cÃ³mo 
resolverlo:

InformaciÃ³n importante:
SegÃºn nuestros tÃ©rminos y condiciones, la acreditaciÃ³n de pagos se realiza en un plazo 
de 24 a 48 horas hÃ¡biles despuÃ©s de finalizado el evento.

Â¿QuÃ© hacer ahora?

1. Verifica el estado del pago:
   - Ingresa a tu panel de prestador
   - SecciÃ³n "Pagos" > "Historial"
   - Confirma la fecha del evento y el estado de acreditaciÃ³n

2. Si ya pasaron mÃ¡s de 48 horas hÃ¡biles:
   EnvÃ­a un correo urgente a wevently.empresa@gmail.com con:
   - Tu ID de prestador
   - NÃºmero de evento
   - Fecha del servicio prestado
   - Monto acordado
   - Captura del panel de pagos

3. InformaciÃ³n bancaria actualizada:
   AsegÃºrate de que tu CBU/CVU estÃ¡ correctamente registrado en tu perfil. Errores en 
   estos datos pueden demorar la acreditaciÃ³n.

Prioridad en tu caso:
Dado que mencionas urgencia, nuestro equipo de soporte priorizarÃ¡ tu caso y te 
responderÃ¡ dentro de las 24 horas siguientes al correo recibido.

No olvides mantener tu perfil y disponibilidad actualizados para evitar inconvenientes.

Estamos comprometidos a resolver tu situaciÃ³n cuanto antes.
```

**Latencia:** 5400 ms (~5.4 segundos)

***

### Caso 3: Consulta fuera de dominio (sin match en Neo4j)

**Input:**

```python
tipousuario = "Organizador"
tipoproblema = "No definido"
solucion = "No definida"
emocion = "alegrÃ­a"
emo_score = 0.62
confianza_fuzzy = 0.27
pregunta = "Me duele la cabeza"
```

**Prompt generado:**

```
Como asistente del sistema Wevently...

Hola estimado organizador! Se detectÃ³ el problema: No definido.
SoluciÃ³n sugerida: No definida.
Por favor responde en un tono positivo, amable y orientado a soluciones.

EmociÃ³n detectada: alegrÃ­a, score emociÃ³n: 0.62, confianza del sistema: 0.27.

Mensaje original: Me duele la cabeza

Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. Cualquier duda 
no dudes en consultarme.

No se encontrÃ³ soluciÃ³n automÃ¡tica, te derivaremos a soporte. wevently.empresa@gmail.com
```

**Respuesta del LLM:**

```
Hola estimado organizador! Lamento que no te sientas bien. Sin embargo, debo 
mencionarte que mi funciÃ³n es ayudarte especÃ­ficamente con consultas relacionadas con 
la plataforma Wevently: eventos, pagos, servicios, proveedores.

Para temas de salud como el que mencionas, te recomiendo:
- Consultar con un profesional mÃ©dico
- Contactar con tu servicio de salud

Si tienes alguna consulta sobre tus eventos o la plataforma Wevently, estarÃ© encantado 
de ayudarte.

Recuerda que puedes gestionar tus eventos desde la secciÃ³n mis eventos. Cualquier duda 
no dudes en consultarme.

Â¡Que te mejores pronto!
```

**ObservaciÃ³n:** El LLM reconoce correctamente que estÃ¡ fuera de su dominio y deriva amablemente sin intentar dar consejos mÃ©dicos.

**Latencia:** 3800 ms (~3.8 segundos)

***

## Diagrama de Flujo de GeneraciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Datos de mÃ³dulos anteriores (MÃ³dulos 3-7)       â”‚
â”‚  - Keywords, EmociÃ³n, Confianza Fuzzy               â”‚
â”‚  - Resultados de Neo4j (mÃºltiples candidatos)      â”‚
â”‚  - Tipo de usuario, CategorÃ­a ML                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  MÃ“DULO 8: LLM          â”‚
        â”‚  (Dual function)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FUNCIÃ“N 1:        â”‚   â”‚ FUNCIÃ“N 2:       â”‚
â”‚ SelecciÃ³n de      â”‚   â”‚ GeneraciÃ³n de    â”‚
â”‚ soluciÃ³n          â”‚   â”‚ respuesta        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚ elegir_mejor_         â”‚ llm.invoke(
         â”‚ solucion_con_llm()    â”‚ prompt_final)
         â”‚                       â”‚
         â–¼                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ Formatear opciones   â”‚        â”‚
â”‚ de Neo4j como texto  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
         â”‚                      â”‚
         â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ Construir prompt de  â”‚        â”‚
â”‚ selecciÃ³n con:       â”‚        â”‚
â”‚ - Mensaje usuario    â”‚        â”‚
â”‚ - CategorÃ­a ML       â”‚        â”‚
â”‚ - EmociÃ³n            â”‚        â”‚
â”‚ - Candidatos         â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
         â”‚                      â”‚
         â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ llm.invoke()         â”‚        â”‚
â”‚ (700-1200 ms)        â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
         â”‚                      â”‚
         â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ Extraer "OpciÃ³n X"   â”‚        â”‚
â”‚ con regex            â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
         â”‚                      â”‚
         â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ Retornar soluciÃ³n    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ elegida              â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Obtener detalles    â”‚
                     â”‚ por rol             â”‚
                     â”‚ (roledetails)       â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Mapear emociÃ³n a    â”‚
                     â”‚ tono (EMOTION_TO_   â”‚
                     â”‚ TONE)               â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Construir prompt    â”‚
                     â”‚ final con:          â”‚
                     â”‚ - InstrucciÃ³n       â”‚
                     â”‚ - Saludo            â”‚
                     â”‚ - Problema/SoluciÃ³n â”‚
                     â”‚ - Tono emocional    â”‚
                     â”‚ - Mensaje original  â”‚
                     â”‚ - Extras por rol    â”‚
                     â”‚ - Post-data         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ llm.invoke()         â”‚
                     â”‚ (4000-5500 ms)       â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Respuesta en         â”‚
                     â”‚ lenguaje natural     â”‚
                     â”‚ (texto estructurado) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


***

## Resultados de Pruebas

### Prueba 1: Consistencia de tono por emociÃ³n

**MetodologÃ­a:** Verificar que el LLM respeta las instrucciones de tono segÃºn emociÃ³n.


| EmociÃ³n | Tono Esperado | Frases en Respuesta | Match |
| :-- | :-- | :-- | :-- |
| enojo | Serio, conciliador | "Entendemos tu frustraciÃ³n...", "Trabajemos juntos..." | âœ… |
| tristeza | Consolador, empÃ¡tico | "Lamentamos mucho...", "Estamos aquÃ­ para apoyarte..." | âœ… |
| alegrÃ­a | Positivo, amable | "Nos alegra ayudarte!", "SerÃ¡ un placer..." | âœ… |
| miedo | Tranquilizador | "No te preocupes...", "Es completamente seguro..." | âœ… |

**ConclusiÃ³n:** El modelo respeta las instrucciones de tono en **100% de los casos**.

***

### Prueba 2: PersonalizaciÃ³n por rol

| Rol | Elemento Esperado | Presente en Respuesta | Match |
| :-- | :-- | :-- | :-- |
| Organizador | "panel de control" / "mis eventos" | âœ… | âœ… |
| Organizador | Saludo "estimado organizador" | âœ… | âœ… |
| Prestador | "mantener tu perfil actualizado" | âœ… | âœ… |
| Prestador | Enfoque operativo | âœ… | âœ… |
| Propietario | "condiciones contractuales" | âœ… | âœ… |

**ConclusiÃ³n:** PersonalizaciÃ³n por rol funciona correctamente.

***

### Prueba 3: Latencia de generaciÃ³n

| Longitud Prompt | Longitud Respuesta | Latencia (seg) | ObservaciÃ³n |
| :-- | :-- | :-- | :-- |
| 150 caracteres | 200 palabras | 3.8 | Respuesta corta |
| 300 caracteres | 350 palabras | 4.2 | Respuesta media |
| 500 caracteres | 500 palabras | 5.4 | Respuesta larga |
| 700 caracteres | 600 palabras | 6.8 | Respuesta muy larga |
| **Promedio** | **~400 palabras** | **4.3 seg** | **53% del tiempo total** |

**ComparaciÃ³n con otros LLMs (estimado):**


| Modelo | Latencia TÃ­pica | ObservaciÃ³n |
| :-- | :-- | :-- |
| `gpt-oss:20b-cloud` (actual) | 4.3 seg | Modelo usado |
| GPT-3.5-turbo (OpenAI) | 1-2 seg | MÃ¡s rÃ¡pido, costo por token |
| Llama-2-13b (local CPU) | 8-12 seg | Sin GPU |
| Mistral-7b (local GPU) | 0.5-1 seg | Requiere GPU potente |


***

### Prueba 4: Coherencia con informaciÃ³n de Neo4j

**Objetivo:** Verificar que el LLM no alucine informaciÃ³n contradictoria.


| Problema Neo4j | SoluciÃ³n Neo4j | Respuesta LLM menciona soluciÃ³n | Match |
| :-- | :-- | :-- | :-- |
| Tarjeta rechazada | Verifique datos... | âœ… Menciona verificaciÃ³n | âœ… |
| Info comisiones | 1% por operaciÃ³n | âœ… Menciona 1% | âœ… |
| Demora acreditaciÃ³n | 24-48hs hÃ¡biles | âœ… Menciona plazo | âœ… |

**ObservaciÃ³n:** El prompt estructurado minimiza alucinaciones. **Coherencia: 100%**.

***

### Prueba 5: Manejo de casos sin soluciÃ³n

**Input:** Consulta fuera de dominio ("me duele la cabeza")

**Comportamiento esperado:** Derivar amablemente sin intentar dar consejos mÃ©dicos.

**Resultado:** âœ… El LLM correctamente:

- Reconoce que no es su dominio
- No da consejos mÃ©dicos
- Sugiere consultar profesional apropiado
- Ofrece ayuda en temas de Wevently

***

## Observaciones y Recomendaciones

### Fortalezas

âœ… **Doble funciÃ³n del LLM:**

1. Selecciona mejor soluciÃ³n de mÃºltiples candidatos
2. Genera respuesta final personalizada

âœ… **PersonalizaciÃ³n triple:** Rol + EmociÃ³n + Contexto crean respuestas Ãºnicas.

âœ… **Tono consistente:** Instrucciones de tono son respetadas por el modelo (100%).

âœ… **Sin alucinaciones crÃ­ticas:** InformaciÃ³n de Neo4j se refleja fielmente.

âœ… **Estructura clara:** Respuestas con bullets, pasos numerados, secciones.

âœ… **FÃ¡cil cambio de modelo:** Arquitectura LangChain permite cambiar proveedor sin refactorizar.

***

### Limitaciones Identificadas
âš ï¸ **Dependencia de servicio externo:** Si Ollama Cloud cae, sistema no puede responder.

âš ï¸ **Sin control de longitud:** Algunas respuestas exceden 500 palabras (demasiado largo).

âš ï¸ **Costo por uso:** Modelos cloud cobran por token (no medido en versiÃ³n acadÃ©mica).

âš ï¸ **Sin memoria conversacional:** Cada mensaje es independiente, no recuerda contexto previo.

âš ï¸ **Doble llamada al LLM:** SelecciÃ³n + GeneraciÃ³n = 2x latencia (aunque necesario).

***

## Mejoras Futuras

### 1. Control de longitud de respuesta

```python
llm = OllamaLLM(
    model="gpt-oss:20b-cloud",
    base_url="https://ollama.com",
    max_tokens=300,  # Limitar longitud
    temperature=0.7  # Controlar creatividad
)
```

**Impacto:** Respuestas mÃ¡s concisas y rÃ¡pidas.

***

### 2. Sistema de fallback local

```python
# LLM primario (cloud)
llm_primary = OllamaLLM(model="gpt-oss:20b-cloud", base_url="https://ollama.com")

# LLM fallback (local)
llm_fallback = OllamaLLM(model="llama2:7b", base_url="http://localhost:11434")

def invoke_llm_con_fallback(prompt):
    try:
        return llm_primary.invoke(prompt)
    except Exception as e:
        logger.warning(f"LLM cloud fallÃ³, usando fallback local: {e}")
        return llm_fallback.invoke(prompt)
```

**Impacto:** Alta disponibilidad incluso si servicio cloud falla.

***

### 3. Memoria conversacional para chat multi-turno

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

def generar_respuesta_con_memoria(pregunta, tipousuario):
    """Genera respuesta considerando historial de conversaciÃ³n."""
    # Recuperar contexto previo
    historial = memory.load_memory_variables({})
    
    prompt_con_contexto = f"""
Historial de conversaciÃ³n:
{historial}

Nueva pregunta: {pregunta}

Responde considerando el contexto previo...
"""
    
    respuesta = llm.invoke(prompt_con_contexto)
    
    # Guardar en memoria
    memory.save_context({"input": pregunta}, {"output": respuesta})
    
    return respuesta
```

**Impacto:** Soporte para conversaciones naturales multi-turno.

***

### 4. CachÃ© de respuestas frecuentes

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def generar_respuesta_cached(prompt_hash):
    """Si el prompt ya fue procesado, retornar de cachÃ©."""
    # ... (implementaciÃ³n con modelo)
    pass

def generar_respuesta_streamlit(pregunta, ...):
    # ... construir prompt ...
    
    prompt_hash = hashlib.md5(prompt_llm.encode()).hexdigest()
    
    # Buscar en cachÃ© primero
    if prompt_hash in cache:
        return cache[prompt_hash], keywords, emocion, confianza
    
    # Generar nueva respuesta
    respuesta = llm.invoke(prompt_llm)
    cache[prompt_hash] = respuesta
    
    return respuesta, keywords, emocion, confianza
```

**Impacto:** Preguntas frecuentes se responden instantÃ¡neamente (~10 ms).

***

### 5. Streaming de respuesta (UX mejorada)

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm_streaming = OllamaLLM(
    model="gpt-oss:20b-cloud",
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# En Streamlit
import streamlit as st

response_placeholder = st.empty()
full_response = ""

for chunk in llm_streaming.stream(prompt_llm):
    full_response += chunk
    response_placeholder.markdown(f"{full_response}â–Œ")

response_placeholder.markdown(full_response)
```

**Impacto:** Usuario ve respuesta generÃ¡ndose en tiempo real (mejor UX).

***

### 6. Post-procesamiento de respuesta

```python
def postprocesar_respuesta(respuesta):
    """Limpia y mejora la respuesta generada."""
    # Limitar longitud
    if len(respuesta) > 1000:
        respuesta = respuesta[:1000] + "..."
    
    # Remover repeticiones
    lineas = respuesta.split('\n')
    lineas_unicas = []
    for linea in lineas:
        if linea not in lineas_unicas:
            lineas_unicas.append(linea)
    respuesta = '\n'.join(lineas_unicas)
    
    # Asegurar cierre amable
    if not any(cierre in respuesta.lower() for cierre in ["saludos", "estamos aquÃ­", "ayudarte"]):
        respuesta += "\n\nÂ¡Estamos aquÃ­ para ayudarte!"
    
    return respuesta
```

**Impacto:** Respuestas mÃ¡s consistentes y profesionales.

***

## Resumen TÃ©cnico

| Aspecto | Valor | ObservaciÃ³n |
| :-- | :-- | :-- |
| **Modelo LLM** | `gpt-oss:20b-cloud` | 20 mil millones de parÃ¡metros |
| **Proveedor** | Ollama Cloud | Servicio cloud https://ollama.com |
| **Latencia promedio** | 4.3 segundos | 53% del tiempo total del sistema |
| **Longitud respuesta** | 200-500 palabras | Variable segÃºn complejidad |
| **Funciones LLM** | 2 | (1) SelecciÃ³n, (2) GeneraciÃ³n |
| **Tono personalizado** | 6 variantes | Por emociÃ³n detectada |
| **Roles soportados** | 3 | Organizador, Prestador, Propietario |
| **Consistencia con Neo4j** | 100% | Sin alucinaciones detectadas |
| **Disponibilidad** | Dependiente de cloud | Sin fallback implementado |
| **% del tiempo total** | 53% | Cuello de botella principal |
| **Costo** | No medido | VersiÃ³n acadÃ©mica |
| **Coherencia contextual** | 100% | 20/20 casos de prueba |


***

## ComparaciÃ³n con Alternativas

### OpciÃ³n actual: Ollama Cloud (`gpt-oss:20b-cloud`)

**Ventajas:**

- âœ… Sin setup de infraestructura
- âœ… Modelo potente (20B parÃ¡metros)
- âœ… FÃ¡cil integraciÃ³n con LangChain

**Desventajas:**

- âŒ Latencia 4-5 segundos
- âŒ Dependencia de servicio externo
- âŒ Costo por token no medido

***

### Alternativa 1: OpenAI GPT-3.5-turbo

**Ventajas:**

- âœ… Latencia 1-2 segundos (mÃ¡s rÃ¡pido)
- âœ… Respuestas de alta calidad
- âœ… API madura y estable

**Desventajas:**

- âŒ Costo alto (\$0.002/1K tokens)
- âŒ Datos enviados a terceros (privacidad)

***

### Alternativa 2: Llama-2-13b (local)

**Ventajas:**

- âœ… Sin costo por uso
- âœ… SoberanÃ­a de datos
- âœ… Sin dependencia de internet

**Desventajas:**

- âŒ Requiere GPU potente
- âŒ Latencia 8-12 seg en CPU
- âŒ Setup complejo

***

### Alternativa 3: Templates estÃ¡ticos + variables

**Ventajas:**

- âœ… Latencia ~10 ms
- âœ… Costo cero
- âœ… 100% predecible

**Desventajas:**

- âŒ Sin flexibilidad lingÃ¼Ã­stica
- âŒ Respuestas repetitivas
- âŒ Menos natural

***

## Arquitectura de IntegraciÃ³n Final

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INPUT DEL USUARIO                            â”‚
â”‚  pregunta: "Mi tarjeta fue rechazada dos veces"     â”‚
â”‚  tipousuario: "Organizador"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ“DULO 7   â”‚       â”‚   MÃ“DULO 3      â”‚
â”‚   NLP      â”‚       â”‚ LÃ³gica Difusa   â”‚
â”‚ (81 ms)    â”‚       â”‚   (12 ms)       â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                         â”‚
    â”‚ keywords, emocion       â”‚ confianza
    â”‚                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MÃ“DULO 4      â”‚
        â”‚    Neo4j        â”‚
        â”‚  (2501 ms)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ [multiple results]
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     MÃ“DULO 8: GENERACIÃ“N (ESTE)         â”‚
        â”‚                                          â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
        â”‚  â”‚ 1. elegir_mejor_solucion_      â”‚     â”‚
        â”‚  â”‚    con_llm()                   â”‚     â”‚
        â”‚  â”‚    - Formatear candidatos      â”‚     â”‚
        â”‚  â”‚    - Construir prompt selecciÃ³nâ”‚     â”‚
        â”‚  â”‚    - llm.invoke() (700-1200ms) â”‚     â”‚
        â”‚  â”‚    - Extraer "OpciÃ³n X" regex  â”‚     â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
        â”‚                 â”‚                        â”‚
        â”‚                 â”‚ (tipoproblema_llm,    â”‚
        â”‚                 â”‚  solucion_llm)        â”‚
        â”‚                 â”‚                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
        â”‚  â”‚ 2. ConstrucciÃ³n prompt final   â”‚     â”‚
        â”‚  â”‚    - roledetails[tipousuario]  â”‚     â”‚
        â”‚  â”‚    - EMOTION_TO_TONE[emocion]  â”‚     â”‚
        â”‚  â”‚    - Saludo + Problema +       â”‚     â”‚
        â”‚  â”‚      SoluciÃ³n + Tono +         â”‚     â”‚
        â”‚  â”‚      Mensaje + Extras          â”‚     â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
        â”‚                 â”‚                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
        â”‚  â”‚ 3. llm.invoke(prompt_final)    â”‚     â”‚
        â”‚  â”‚    (4000-5500 ms)              â”‚     â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
        â”‚                 â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    OUTPUT FINAL                         â”‚
        â”‚  Respuesta en lenguaje natural:         â”‚
        â”‚  "Hola estimado organizador! Entendemos â”‚
        â”‚   tu frustraciÃ³n... [pasos] ..."       â”‚
        â”‚                                          â”‚
        â”‚  Latencia total: ~7.96 segundos         â”‚
        â”‚  (LLM = 53% del tiempo)                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


***

## ConclusiÃ³n

El **MÃ³dulo 8: IntegraciÃ³n Generativa** es el **componente de interfaz humana del sistema**, transformando datos frÃ­os y estructurados en conversaciones cÃ¡lidas y empÃ¡ticas.

### âœ… Logros clave:

1. **Doble funciÃ³n del LLM:**
    - Selecciona mejor soluciÃ³n de mÃºltiples candidatos
    - Genera respuesta final personalizada
2. **PersonalizaciÃ³n triple:** Rol + EmociÃ³n + Contexto crean respuestas Ãºnicas
3. **Respuestas estructuradas:** Pasos numerados, bullets, secciones claras
4. **Tono consistente:** Instrucciones respetadas en 100% de casos
5. **Sin alucinaciones crÃ­ticas:** Fidelidad a informaciÃ³n de Neo4j
6. **Arquitectura flexible:** Cambiar de proveedor LLM es trivial con LangChain

### âš ï¸ Trade-offs aceptados:

- **Latencia dominante (53%):** Pero necesaria para generar respuestas naturales y empÃ¡ticas
- **Dependencia externa:** Cloud service, pero con arquitectura preparada para fallback
- **Costo por token:** No medido en versiÃ³n acadÃ©mica, pero proyectable


### ğŸ¯ Valor del mÃ³dulo:

Aunque consume el 53% del tiempo total de respuesta, su valor es **insustituible**: convierte un sistema experto tÃ©cnico en un asistente conversacional que los usuarios perciben como inteligente y humano.

**Sin este mÃ³dulo**, el sistema solo podrÃ­a retornar datos estructurados como JSON, perdiendo completamente la experiencia conversacional natural.

***

### ğŸš€ PrÃ³ximos pasos para producciÃ³n:

1. Implementar fallback local para alta disponibilidad
2. Agregar cachÃ© para preguntas frecuentes (reducir latencia y costo)
3. Monitorear costo por token en ambiente real
4. Considerar streaming para mejor UX
5. Evaluar modelos alternativos (GPT-3.5, Llama-2) segÃºn mÃ©tricas reales

***


**Ãšltima actualizaciÃ³n:** 2025-11-17
**VersiÃ³n:** 2.0 
**Estado:** âœ… Implementado con doble funciÃ³n (selecciÃ³n + generaciÃ³n)

***